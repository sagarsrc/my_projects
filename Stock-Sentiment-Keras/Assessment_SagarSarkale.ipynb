{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import Adamax\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Don't change these functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function loads the data\n",
    "'''\n",
    "def loadData(filename):\n",
    "    rawData = pd.read_csv(filename)\n",
    "    closePrice = np.array(rawData.loc[:, [' CLOSE']])\n",
    "    return   closePrice[:,0]\n",
    "\n",
    "'''\n",
    "This function extracts X and Y data where X is the price for past n days\n",
    "n = lookback for this func., Y is price for the next m days.\n",
    "m = f_horizon for this func\n",
    "NOTE: WE ARE NOT USING FUNCTION FOR THIS EXERCISE. THIS IS JUST FOR YOUR LEARNING\n",
    "'''\n",
    "def create_dataset(closePrice,look_back,f_horizon):    \n",
    "    dataX = []\n",
    "    dataY = []        \n",
    "    for i in range(0,len(closePrice)-look_back-f_horizon,1):\n",
    "        a = closePrice[i:i+look_back]\n",
    "        b = closePrice[i+look_back:i+look_back + f_horizon]               \n",
    "        dataX.append(a.tolist())\n",
    "        dataY.append(b.tolist())\n",
    "    return dataX, dataY\n",
    "\n",
    "'''\n",
    "This function extracts X and Y data where X is the price for past n days\n",
    "n = lookback for this func., Y is the labels that can have 3 values -1, 0, 1\n",
    "-1 denotes a drop in stock price, 0 denotes no change, and 1 denotes an increase in stock price. \n",
    "'''\n",
    "def create_dataset_discrete(closePrice,look_back):    \n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    dataY_raw = []\n",
    "        \n",
    "    for i in range(0,len(closePrice)-look_back-1,1):\n",
    "        a = closePrice[i:i+look_back]\n",
    "        b = (closePrice[i+look_back] - closePrice[i+look_back-1])/ (closePrice[i+look_back-1]) * 100\n",
    "        # b is the percentage change in price for the next day\n",
    "        \n",
    "        labels = 0\n",
    "        if b > 0.25:\n",
    "            labels = 1\n",
    "        elif b < -0.25:\n",
    "            labels = -1\n",
    "        dataX.append(a.tolist())\n",
    "        dataY.append(labels)\n",
    "        dataY_raw.append(b)\n",
    "        \n",
    "    return dataX, dataY, dataY_raw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You may want to code some of these functions to prepare your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Normalized Dataset\n",
    "def create_normalized_dataset(dataX, dataY):\n",
    "    # you may want to add your code to normalize the dataset\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(dataX)\n",
    "    dataX = scaler.transform(dataX)\n",
    "    return dataX\n",
    "    \n",
    "\n",
    "# Create Normalized Dataset\n",
    "def preprocessData(dataX, dataY):\n",
    "    # you may want to add your code to preprocess data \n",
    "    onehot_encoded = list()\n",
    "    for i in dataY:\n",
    "        if i == 0:\n",
    "            onehot_encoded.append([0,0,1])\n",
    "        if i == 1:\n",
    "            onehot_encoded.append([0,1,0])\n",
    "        if i == -1:\n",
    "            onehot_encoded.append([1,0,0])\n",
    "    \n",
    "    dataX = dataX.reshape(dataX.shape[0],dataX.shape[1],1)\n",
    "        \n",
    "    return dataX,onehot_encoded\n",
    "            \n",
    "# Create Normalized Dataset\n",
    "def extractFeatures(dataX, dataY):\n",
    "    # you may generate some features such as moving averages, Relative strength index etc    \n",
    "    pass\n",
    "\n",
    "\n",
    "# This func. create train and test data. Given the entire dataset\n",
    "# Note that the code doesn't shuffle the data\n",
    "def createData_TrainTest(dataX, dataY, percent_train_data):\n",
    "    num_training_data = int (len(dataY)*percent_train_data)\n",
    "    idx = np.arange(0 , len(dataY))\n",
    "    #np.random.shuffle(idx)  # Shuffling can provide future info.\n",
    "    # Extract Test and Train data\n",
    "    trainX = [dataX[i] for i in idx[0:num_training_data]]\n",
    "    testX  = [dataX[i] for i in idx[num_training_data:]]\n",
    "    trainY = [dataY[i] for i in idx[0:num_training_data]]\n",
    "    testY =  [dataY[i] for i in idx[num_training_data:]]  \n",
    "    \n",
    "    return trainX, trainY, testX, testY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function is just an example for naive bayes implementation.\n",
    "It takes the training features and labels as input and learns a\n",
    "decision tree model using SKLearn's naive bayes algorithm. It runs 10-fold\n",
    "cross-validation on the training data to identify the best depth.\n",
    "\n",
    "PLEASE DONT USE THIS FUNCTION. THIS FUNCTION IS BY NO MEAN A GUIDELINE FOR YOUR CODE.\n",
    "THIS FUNCTION IS ONLY INCLUDED FOR YOUR LEARNING\n",
    "'''\n",
    "def learn_naive_bayes(X, y):\n",
    "    # This list tracks the learned decision tree with the best accuracy\n",
    "    best_model = [ None, float(\"-inf\") ]\n",
    "    # Create the object that will split the training set into training and\n",
    "    # validation sets\n",
    "    kf = KFold(n_splits=10)\n",
    "    # Iterate over each of the 10 splits on the data set\n",
    "    for train, test in kf.split(X):\n",
    "        # Pull out the features and labels that will be used to train this model\n",
    "        train_X = [ X[dp] for dp in train ]\n",
    "        train_y = [ y[dp] for dp in train ]\n",
    "        # Pull out the features and labels that will be used to validate this\n",
    "        # model\n",
    "        valid_X = [ X[dp] for dp in test ]\n",
    "        valid_y = [ y[dp] for dp in test ]\n",
    "        # Create the decision tree object\n",
    "        clf = GaussianNB()\n",
    "        # Learn the model on the training data that will be used for this\n",
    "        # fold\n",
    "        clf = clf.fit(train_X, train_y)\n",
    "        # Evaluate the learned model on the validation set\n",
    "        accuracy = clf.score(valid_X, valid_y)\n",
    "        # Check whether or not this learned model is the most accuracy model\n",
    "        if accuracy > best_model[1]:\n",
    "            # Update best_model so that it holds this learned model and its\n",
    "            # associated accuracy and hyper-parameter information\n",
    "            best_model = [ clf, accuracy ]\n",
    "    return best_model\n",
    "\n",
    "\n",
    "def MLClassifier(trainX,trainY,testX,testY, batch_size, batch_per_step, epochs, training_steps):\n",
    "    # code as many classifier as you want. You can have as many functions for classifier as you need\n",
    "    # you can also write Neural network function if you are interested\n",
    "    def LSTM_keras():\n",
    "\n",
    "        # LSTM keras model\n",
    "        model = Sequential()\n",
    "\n",
    "        # LSTM layer1\n",
    "        model.add(LSTM(128, return_sequences=True,input_shape=(trainX.shape[1], trainX.shape[2]),dropout=0.2, recurrent_dropout=0.2))\n",
    "        # model.add(LSTM(128, return_sequences=True,input_shape=(batch_size, trainX.shape[2]),dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "        # LSTM layer2\n",
    "        model.add(LSTM(128,return_sequences=True, dropout=0.2, recurrent_dropout=0.2,))\n",
    "\n",
    "        # LSTM layer3\n",
    "        model.add(LSTM(256,dropout=0.2, activation='relu' ,recurrent_dropout=0.2))\n",
    "\n",
    "        # Dense Layer\n",
    "        model.add(Dense(64,activation='relu'))\n",
    "\n",
    "        # Final Layer output 3 classes\n",
    "        model.add(Dense(3,activation='softmax'))\n",
    "\n",
    "        # optimizer\n",
    "        adamax = Adamax(lr=0.0035, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "\n",
    "        compiled_model = model.compile(optimizer=adamax,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "        return compiled_model\n",
    "\n",
    "    \n",
    "            \n",
    "    LSTM_model = LSTM_keras()\n",
    "\n",
    "    # network parameters\n",
    "    \n",
    "    batch_size = batch_size\n",
    "    batch_per_step = batch_per_step\n",
    "    epochs = epochs\n",
    "    training_steps = training_steps\n",
    "    \n",
    "    \n",
    "    print(\"No. of training Steps required: \",training_steps)\n",
    "\n",
    "    \n",
    "    c=0\n",
    "    \n",
    "    # train the model\n",
    "    for i in range(0,trainY.shape[0],batch_size):\n",
    "        print(\"\\nTraining Step: \", c)\n",
    "        \n",
    "        batch_x,batch_y = get_batches(i,batch_size,trainX,trainY)\n",
    "        LSTM_model.fit(batch_x,batch_y,batch_size=batch_per_step,epochs=epochs)\n",
    "\n",
    "        c+=1\n",
    "    \n",
    "    # returns trained model\n",
    "    return LSTM_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize():\n",
    "    # write your visualization tools here: you can code as many functions as you would like\n",
    "    pass\n",
    "\n",
    "def hist_visualize(dataY, x_label_name,y_label_name, figure_title):\n",
    "    n, bins, patches = plt.hist(x=dataY, bins='auto', color='#0504aa', alpha=0.7, rwidth=0.85)\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    plt.xlabel(x_label_name)\n",
    "    plt.ylabel(y_label_name)\n",
    "    plt.title(figure_title)\n",
    "    plt.text(23, 45, r'$\\mu=15, b=3$')\n",
    "    plt.ylim()\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# helper function: That doesn't fall under any of the above categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(i,batch_size,trainX,trainY):\n",
    "    batch_x = trainX[i:i+batch_size]\n",
    "    batch_y = trainY[i:i+batch_size]\n",
    "    \n",
    "    return batch_x,batch_y\n",
    "\n",
    "def to_np_array(trainX, trainY, testX, testY):\n",
    "    trainX, trainY, testX, testY = np.array(trainX),np.array(trainY),np.array(testX),np.array(testY)\n",
    "    \n",
    "    return trainX, trainY, testX, testY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Code Here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Variable\n",
    "filename = 'stock_data.txt' # file containing stock data \n",
    "look_back = 300  # number of days to lookback to predict the future\n",
    "percent_train_data = .7 # fraction of training data, .3 = fraction of test data.\n",
    "                        # we are not dividing into train-test-validation for this problem\n",
    "\n",
    "closePrice= loadData(filename) # Load closing price of stock from the file. Check below visualization to understand\n",
    "'''\n",
    "extracts X and y data,  where X is the price for past n ( = lookback) days\n",
    "Y is the labels: -1, 0, 1\n",
    "-1 denotes a drop in stock price, 0 denotes no change, and 1 denotes an increase in stock price. \n",
    "'''\n",
    "dataX, dataY, dataY_raw = create_dataset_discrete(closePrice,look_back) # dataY_raw in not useful for this exercise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start writing your code here: Below added code is just a guideline for your understanding\n",
    "\n",
    "# you may want to code these functions above. You are allowed to use/ change/ reorder below structure\n",
    "# dataX is a time series data.\n",
    "extractFeatures(dataX, dataY)  \n",
    "dataX = create_normalized_dataset(dataX, dataY)\n",
    "dataX, dataY = preprocessData(dataX, dataY)\n",
    "\n",
    "\n",
    "trainX, trainY, testX, testY = createData_TrainTest(dataX, dataY, percent_train_data) # split training-testing data\n",
    "trainX, trainY, testX, testY = to_np_array(trainX, trainY, testX, testY)\n",
    "\n",
    "# Check the function above. We are not shuffling the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call to ML Classifier in this case Keras LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of training Steps required:  15\n",
      "\n",
      "Training Step:  0\n",
      "Epoch 1/2\n",
      "300/300 [==============================] - 19s 63ms/step - loss: 1.0913 - acc: 0.3800\n",
      "Epoch 2/2\n",
      "300/300 [==============================] - 12s 39ms/step - loss: 1.0823 - acc: 0.4000\n",
      "\n",
      "Training Step:  1\n",
      "Epoch 1/2\n",
      "300/300 [==============================] - 11s 38ms/step - loss: 1.0838 - acc: 0.3800\n",
      "Epoch 2/2\n",
      "300/300 [==============================] - 12s 38ms/step - loss: 1.0852 - acc: 0.3867\n",
      "\n",
      "Training Step:  2\n",
      "Epoch 1/2\n",
      "300/300 [==============================] - 12s 39ms/step - loss: 1.1124 - acc: 0.3200\n",
      "Epoch 2/2\n",
      "300/300 [==============================] - 11s 38ms/step - loss: 1.1049 - acc: 0.3367\n",
      "\n",
      "Training Step:  3\n",
      "Epoch 1/2\n",
      "300/300 [==============================] - 12s 41ms/step - loss: 1.0962 - acc: 0.3600\n",
      "Epoch 2/2\n",
      "300/300 [==============================] - 15s 49ms/step - loss: 1.0971 - acc: 0.3600\n",
      "\n",
      "Training Step:  4\n",
      "Epoch 1/2\n",
      "300/300 [==============================] - 12s 41ms/step - loss: 1.1121 - acc: 0.2333\n",
      "Epoch 2/2\n",
      "300/300 [==============================] - 11s 38ms/step - loss: 1.1054 - acc: 0.2333\n",
      "\n",
      "Training Step:  5\n",
      "Epoch 1/2\n",
      "300/300 [==============================] - 13s 44ms/step - loss: 1.0989 - acc: 0.3167\n",
      "Epoch 2/2\n",
      "300/300 [==============================] - 12s 41ms/step - loss: 1.0986 - acc: 0.3467\n",
      "\n",
      "Training Step:  6\n",
      "Epoch 1/2\n",
      "300/300 [==============================] - 12s 39ms/step - loss: 1.0977 - acc: 0.3433\n",
      "Epoch 2/2\n",
      "300/300 [==============================] - 11s 38ms/step - loss: 1.0972 - acc: 0.3567\n",
      "\n",
      "Training Step:  7\n",
      "Epoch 1/2\n",
      "300/300 [==============================] - 12s 39ms/step - loss: 1.0965 - acc: 0.3767\n",
      "Epoch 2/2\n",
      "300/300 [==============================] - 11s 38ms/step - loss: 1.0941 - acc: 0.3767\n",
      "\n",
      "Training Step:  8\n",
      "Epoch 1/2\n",
      "300/300 [==============================] - 12s 39ms/step - loss: 1.1033 - acc: 0.3300\n",
      "Epoch 2/2\n",
      "300/300 [==============================] - 11s 38ms/step - loss: 1.1023 - acc: 0.3300\n",
      "\n",
      "Training Step:  9\n",
      "Epoch 1/2\n",
      "300/300 [==============================] - 12s 41ms/step - loss: 1.1059 - acc: 0.2667\n",
      "Epoch 2/2\n",
      "300/300 [==============================] - 12s 41ms/step - loss: 1.1018 - acc: 0.2667\n",
      "\n",
      "Training Step:  10\n",
      "Epoch 1/2\n",
      "300/300 [==============================] - 11s 38ms/step - loss: 1.0992 - acc: 0.3167\n",
      "Epoch 2/2\n",
      "300/300 [==============================] - 11s 38ms/step - loss: 1.0993 - acc: 0.3000\n",
      "\n",
      "Training Step:  11\n",
      "Epoch 1/2\n",
      "300/300 [==============================] - 11s 38ms/step - loss: 1.0987 - acc: 0.3133\n",
      "Epoch 2/2\n",
      "300/300 [==============================] - 11s 38ms/step - loss: 1.0988 - acc: 0.3233\n",
      "\n",
      "Training Step:  12\n",
      "Epoch 1/2\n",
      "300/300 [==============================] - 12s 38ms/step - loss: 1.0987 - acc: 0.3433\n",
      "Epoch 2/2\n",
      "300/300 [==============================] - 12s 41ms/step - loss: 1.0987 - acc: 0.2900\n",
      "\n",
      "Training Step:  13\n",
      "Epoch 1/2\n",
      "300/300 [==============================] - 12s 39ms/step - loss: 1.0973 - acc: 0.4467\n",
      "Epoch 2/2\n",
      "300/300 [==============================] - 12s 40ms/step - loss: 1.0947 - acc: 0.5067\n",
      "\n",
      "Training Step:  14\n",
      "Epoch 1/2\n",
      "300/300 [==============================] - 13s 44ms/step - loss: 1.1033 - acc: 0.2300\n",
      "Epoch 2/2\n",
      "300/300 [==============================] - 12s 41ms/step - loss: 1.1037 - acc: 0.2300\n",
      "\n",
      "Training Step:  15\n",
      "Epoch 1/2\n",
      "237/237 [==============================] - 11s 46ms/step - loss: 1.1007 - acc: 0.2743\n",
      "Epoch 2/2\n",
      "237/237 [==============================] - 10s 44ms/step - loss: 1.0999 - acc: 0.2743\n"
     ]
    }
   ],
   "source": [
    "# batch_per_step can be set to lower value for better accuracy\n",
    "# epochs can be increased for better generalisatoin\n",
    "# \n",
    "batch_size = 200\n",
    "batch_per_step = 20\n",
    "epochs = 2\n",
    "training_steps = trainY.shape[0]//batch_size\n",
    "\n",
    "model = MLClassifier(trainX,trainY,testX,testY, batch_size, batch_per_step, epochs, training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Exploration: This shows that the class distribution is balanced and\n",
    "# we have almost the same number of labels in all the classess\n",
    "hist_visualize(dataY, 'Class Labels','frequency', 'class_distribution') # class distribution is good\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THESE Three LINES BELOW ARE JUST EXAMPLES/ please delete or comment these lines in your final submission\n",
    "best_naive_bayes = learn_naive_bayes(trainX, trainY)  \n",
    "naive_bayes_accuracy = best_naive_bayes[0].score(testX, testY)\n",
    "naive_bayes_accuracy\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
