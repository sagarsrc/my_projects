{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.9.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST/t10k-labels-idx1-ubyte.gz\n",
      "Size of:\n",
      "- Training-set:\t\t55000\n",
      "- Test-set:\t\t10000\n",
      "- Validation-set:\t5000\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "data = input_data.read_data_sets('data/MNIST/', one_hot=True)\n",
    "\n",
    "# insights of dataset\n",
    "print(\"Size of:\")\n",
    "print(\"- Training-set:\\t\\t{}\".format(len(data.train.labels)))\n",
    "print(\"- Test-set:\\t\\t{}\".format(len(data.test.labels)))\n",
    "print(\"- Validation-set:\\t{}\".format(len(data.validation.labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholder for input images\n",
    "\"\"\"\n",
    "Why reshape to -1? -1 is a placeholder that says \n",
    "\"adjust as necessary to match the size needed for the full tensor.\" \n",
    "It's a way of making the code be independent of the input batch size, \n",
    "so that you can change your pipeline and not have to adjust the batch size \n",
    "everywhere in the code.\n",
    "\"\"\"\n",
    "x = tf.placeholder(tf.float32, [None,28*28], name='X')\n",
    "x_image = tf.reshape(x,[-1,28,28,1])\n",
    "\n",
    "# placeholder for true y labels associated with images\n",
    "y_true = tf.placeholder(tf.float32, [None, 10], name='Y')\n",
    "y_true_cls = tf.argmax(y_true, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convolutional layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights shape = [height_of_filter, width_of_filter, no_ip_channels, depth_of_layer]\n",
    "# here depth_of_layer is no. of feature maps\n",
    "# using 5X5 filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add:0' shape=(?, 28, 28, 6) dtype=float32>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_conv1 = tf.Variable(tf.truncated_normal(shape=[5,5,1,6]))\n",
    "b_conv1 = tf.Variable(tf.constant(0.01, shape=[6]))\n",
    "\n",
    "conv1 = tf.nn.conv2d(x_image, w_conv1, strides=[1,1,1,1], padding='SAME')+ b_conv1\n",
    "# note that output of conv1 is 6 28X28 images\n",
    "conv1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## relu layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu:0' shape=(?, 28, 28, 6) dtype=float32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu1 = tf.nn.relu(conv1)\n",
    "relu1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pooling layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MaxPool:0' shape=(?, 28, 28, 6) dtype=float32>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kernel size [no_of_kernels, kernel_height, kernel_width, no_of_channels]\n",
    "pool1 = tf.nn.max_pool(value=relu1, ksize=[1,2,2,1], strides=[1,1,1,1], padding='SAME')\n",
    "pool1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convolutional layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_1:0' shape=(?, 28, 28, 32) dtype=float32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# <tf.Tensor 'Relu_2:0' shape=(?, 28, 28, 6) dtype=float32>\n",
    "# we have 6 feature maps of 28 by 28\n",
    "# notice : w_conv2 = tf.Variable(tf.truncated_normal(shape=[2,2,6,32]))\n",
    "# now we convolute using 2 by 2 filter and get 32 feature maps\n",
    "\n",
    "w_conv2 = tf.Variable(tf.truncated_normal(shape=[2,2,6,32]))\n",
    "b_conv2 = tf.Variable(tf.constant(0.05, shape=[32]))\n",
    "\n",
    "conv2 = tf.nn.conv2d(pool1, w_conv2, strides=[1,1,1,1], padding='SAME')+ b_conv2\n",
    "# note that output of conv1 is 6 28X28 images\n",
    "conv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## relu layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_1:0' shape=(?, 28, 28, 32) dtype=float32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu2 = tf.nn.relu(conv2)\n",
    "relu2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pooling layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MaxPool_1:0' shape=(?, 14, 14, 32) dtype=float32>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kernel size [no_of_kernels, kernel_height, kernel_width, no_of_channels]\n",
    "# notice strides=[1,2,2,1]\n",
    "# after sliding 2 cells at a time dimensionality reduction can be seen\n",
    "# pool2 <tf.Tensor 'MaxPool_2:0' shape=(?, 14, 14, 32) dtype=float32>\n",
    "pool2 = tf.nn.max_pool(value=relu2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "pool2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fully connected network 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_1:0' shape=(?, 6272) dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fcn1 fully connected network\n",
    "# now we flatten all the feature maps and\n",
    "# send it to fully connected network\n",
    "\n",
    "fcn_mat1 = tf.reshape(pool2,shape=[-1,14*14*32])\n",
    "fcn_mat1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_2:0' shape=(?, 256) dtype=float32>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weights and biases for fcn1\n",
    "w_fcn1 = tf.Variable(tf.truncated_normal(shape=[14*14*32,256],stddev=0.5))\n",
    "b_fcn1 = tf.Variable(tf.constant(value=0.3 , shape=[256]))\n",
    "\n",
    "# mat mul operation\n",
    "mat_mul_fcn1 = tf.matmul(fcn_mat1,w_fcn1) + b_fcn1\n",
    "mat_mul_fcn1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## relu layer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_2:0' shape=(?, 256) dtype=float32>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu3 = tf.nn.relu(mat_mul_fcn1)\n",
    "relu3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fully connected network 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_3:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weights and biases for fcn1\n",
    "w_fcn2 = tf.Variable(tf.truncated_normal(shape=[256,10],stddev=0.5))\n",
    "b_fcn2 = tf.Variable(tf.constant(value=0.05 , shape=[10]))\n",
    "\n",
    "# mat mul operation\n",
    "mat_mul_fcn2 = tf.matmul(relu3,w_fcn2) + b_fcn2\n",
    "mat_mul_fcn2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## applying softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax will generate probabilities \n",
    "# or scores of each digit we need to choose max probability\n",
    "# axis = 1 meaning in columns\n",
    "\n",
    "y_pred = tf.nn.softmax(mat_mul_fcn2)\n",
    "y_pred_cls = tf.argmax(y_pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=mat_mul_fcn2, labels=y_true)\n",
    "cost =tf.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimiser and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using adam optimiser\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "\n",
    "# this is to convert integer to float32 for accuracy check\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setting parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed : Time usage 41 seconds\n",
      "\tAccuracy:\n",
      "\t- Training Accuracy:\t0.17260000055655836\n",
      "\t- Validation Accuracy:\t0.3336000144481659\n",
      "\t- Test Accuracy:\t0.30000001192092896\n",
      "Epoch 2 completed : Time usage 39 seconds\n",
      "\tAccuracy:\n",
      "\t- Training Accuracy:\t0.4781999981403351\n",
      "\t- Validation Accuracy:\t0.5820000171661377\n",
      "\t- Test Accuracy:\t0.5799999833106995\n",
      "Epoch 3 completed : Time usage 41 seconds\n",
      "\tAccuracy:\n",
      "\t- Training Accuracy:\t0.6474000012874603\n",
      "\t- Validation Accuracy:\t0.6904000043869019\n",
      "\t- Test Accuracy:\t0.6299999952316284\n",
      "Epoch 4 completed : Time usage 39 seconds\n",
      "\tAccuracy:\n",
      "\t- Training Accuracy:\t0.7330000007152557\n",
      "\t- Validation Accuracy:\t0.7576000094413757\n",
      "\t- Test Accuracy:\t0.6899999976158142\n",
      "Epoch 5 completed : Time usage 40 seconds\n",
      "\tAccuracy:\n",
      "\t- Training Accuracy:\t0.7862999957799911\n",
      "\t- Validation Accuracy:\t0.7964000105857849\n",
      "\t- Test Accuracy:\t0.8299999833106995\n",
      "Total time required to train:  202.3239517211914\n"
     ]
    }
   ],
   "source": [
    "train_acc_array = []\n",
    "valid_acc_array = []\n",
    "test_acc_array = []\n",
    "pred_results = []\n",
    "\n",
    "# initialize all global variables\n",
    "sess.run(tf.initializers.global_variables())\n",
    "total_time_t0 = time.time()\n",
    "\n",
    "for epochs in range(num_epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_accuracy = 0\n",
    "\n",
    "    # Training-set consists: 55000\n",
    "    # Using only 50 epochs\n",
    "    # Training only 10 batches per epoch\n",
    "    # Therefore total of 50*10 iterations\n",
    "    for batch in range(100):\n",
    "\n",
    "        # get batch of images and labels\n",
    "        x_batch, y_true_batch = data.train.next_batch(batch_size)\n",
    "\n",
    "        # set feed_dict_train\n",
    "        feed_dict_train = {x:x_batch, y_true:y_true_batch}\n",
    "\n",
    "        # feed dictionary into optimiser for training\n",
    "        sess.run(optimizer,feed_dict=feed_dict_train)\n",
    "\n",
    "        # Calculate the cumulative accuracy on the batch of training data\n",
    "        train_accuracy += sess.run(accuracy, feed_dict=feed_dict_train)\n",
    "        # print(\"*** Train_accuracy_batch\",train_accuracy)\n",
    "\n",
    "\n",
    "    # get validation sets\n",
    "    v_img, v_labels = data.validation.next_batch(100)\n",
    "    # feed validation set\n",
    "    vali_accuracy = sess.run(accuracy, feed_dict={x:data.validation.images, y_true:data.validation.labels})\n",
    "\n",
    "    # get test sets\n",
    "    tst_imgs, tst_labels = data.test.next_batch(100)\n",
    "    # check over test set\n",
    "    test_accuracy = sess.run(accuracy, feed_dict={x:tst_imgs, y_true:tst_labels})\n",
    "    \n",
    "    # printing test images and their labels\n",
    "    pred_batch_x, pred_batch_y = data.test.next_batch(5)\n",
    "    pred_results.append([pred_batch_x,sess.run(y_pred,feed_dict={x:pred_batch_x})])\n",
    "\n",
    "    # average train accuracy\n",
    "    # train_accuracy /= int(len(data.train.labels)/batch_size)\n",
    "    train_accuracy /= 100\n",
    "    \n",
    "    # appending in arrays for plotting\n",
    "    train_acc_array.append(train_accuracy)\n",
    "    valid_acc_array.append(vali_accuracy)\n",
    "    test_acc_array.append(test_accuracy)\n",
    "    \n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"Epoch \"+str(epochs+1)+\" completed : Time usage \"+str(int(end_time-start_time))+\" seconds\")\n",
    "    print(\"\\tAccuracy:\")\n",
    "    print (\"\\t- Training Accuracy:\\t{}\".format(train_accuracy))\n",
    "    print (\"\\t- Validation Accuracy:\\t{}\".format(vali_accuracy))\n",
    "    print (\"\\t- Test Accuracy:\\t{}\".format(test_accuracy))\n",
    "\n",
    "total_time_tn = time.time()\n",
    "print(\"Total time required to train: \",total_time_tn-total_time_t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_results[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5\n",
      "0\n",
      "8\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, axarr = plt.subplots(2,2)\n",
    "axarr[0,0].imshow(np.reshape(pred_results[4][0][0],(28,28)),cmap='gray');print(np.argmax(pred_results[4][1][0]))\n",
    "axarr[0,1].imshow(np.reshape(pred_results[4][0][1],(28,28)),cmap='gray');print(np.argmax(pred_results[4][1][1]))\n",
    "axarr[1,0].imshow(np.reshape(pred_results[4][0][2],(28,28)),cmap='gray');print(np.argmax(pred_results[4][1][2]))\n",
    "axarr[1,1].imshow(np.reshape(pred_results[4][0][3],(28,28)),cmap='gray');print(np.argmax(pred_results[4][1][3]))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
